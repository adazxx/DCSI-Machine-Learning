{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of raw data set: (569, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['ID','Class',\n",
    "          'radius_mean','radius_SE','radius_worst',\n",
    "         'texture_mean','texture_SE','texture_worst',\n",
    "         'perimeter_mean','perimeter_SE','perimeter_worst',\n",
    "         'area_mean','area_SE','area_worst',\n",
    "         'smooth_mean','smooth_SE','smooth_worst',\n",
    "         'compact_mean','compact_SE','compact_worst',\n",
    "         'concavity_mean','concavity_SE','concavity_worst',\n",
    "         'concave_points_mean','concave_points_SE','concave_points_worst',\n",
    "         'symmetry_mean','symmetry_SE','symmetry_worst',\n",
    "        'fractal_dim_mean','fractal_dim_SE','fractal_dim_worst']\n",
    "df = pd.read_csv(r'../data/wdbc.data', names = names)\n",
    "\n",
    "print('shape of raw data set:',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>radius_SE</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>texture_SE</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>perimeter_SE</th>\n",
       "      <th>...</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>concave_points_SE</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>symmetry_SE</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dim_mean</th>\n",
       "      <th>fractal_dim_SE</th>\n",
       "      <th>fractal_dim_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Class  radius_mean  radius_SE  radius_worst  texture_mean  \\\n",
       "0    842302     M        17.99      10.38        122.80        1001.0   \n",
       "1    842517     M        20.57      17.77        132.90        1326.0   \n",
       "2  84300903     M        19.69      21.25        130.00        1203.0   \n",
       "3  84348301     M        11.42      20.38         77.58         386.1   \n",
       "4  84358402     M        20.29      14.34        135.10        1297.0   \n",
       "\n",
       "   texture_SE  texture_worst  perimeter_mean  perimeter_SE  ...  \\\n",
       "0     0.11840        0.27760          0.3001       0.14710  ...   \n",
       "1     0.08474        0.07864          0.0869       0.07017  ...   \n",
       "2     0.10960        0.15990          0.1974       0.12790  ...   \n",
       "3     0.14250        0.28390          0.2414       0.10520  ...   \n",
       "4     0.10030        0.13280          0.1980       0.10430  ...   \n",
       "\n",
       "   concavity_worst  concave_points_mean  concave_points_SE  \\\n",
       "0            25.38                17.33             184.60   \n",
       "1            24.99                23.41             158.80   \n",
       "2            23.57                25.53             152.50   \n",
       "3            14.91                26.50              98.87   \n",
       "4            22.54                16.67             152.20   \n",
       "\n",
       "   concave_points_worst  symmetry_mean  symmetry_SE  symmetry_worst  \\\n",
       "0                2019.0         0.1622       0.6656          0.7119   \n",
       "1                1956.0         0.1238       0.1866          0.2416   \n",
       "2                1709.0         0.1444       0.4245          0.4504   \n",
       "3                 567.7         0.2098       0.8663          0.6869   \n",
       "4                1575.0         0.1374       0.2050          0.4000   \n",
       "\n",
       "   fractal_dim_mean  fractal_dim_SE  fractal_dim_worst  \n",
       "0            0.2654          0.4601            0.11890  \n",
       "1            0.1860          0.2750            0.08902  \n",
       "2            0.2430          0.3613            0.08758  \n",
       "3            0.2575          0.6638            0.17300  \n",
       "4            0.1625          0.2364            0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Monte-Carlo Simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_random_split(i):\n",
    "    b_df = df.loc[df['Class'] == 'B'].sample(frac = 0.2, random_state = i)\n",
    "    m_df = df.loc[df['Class'] == 'M'].sample(frac = 0.2, random_state = i)\n",
    "    df_test = b_df.append(m_df)\n",
    "    df_train = df.drop(df_test.index).reset_index(drop = True)\n",
    "    df_test = df_test.reset_index(drop = True)\n",
    "    \n",
    "    X_train = Normalizer().fit_transform(df_train.drop(['ID', 'Class'], axis = 1))\n",
    "    y_train = LabelBinarizer().fit_transform(df_train['Class'])\n",
    "    X_test = Normalizer().fit_transform(df_test.drop(['ID', 'Class'], axis = 1))\n",
    "    y_test = LabelBinarizer().fit_transform(df_test['Class'])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rep = ['train_accuracy','train_precision','train_recall','train_f1','train_auc']\n",
    "test_rep = ['test_accuracy','test_precision','test_recall','test_f1','test_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Supervised Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "\n",
    "train_accuracy = []\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "train_f1 = []\n",
    "train_auc = []\n",
    "    \n",
    "test_accuracy = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "test_auc = []\n",
    "\n",
    "for i in range(0,30):\n",
    "    X_train, y_train, X_test, y_test = normalized_random_split(i)\n",
    "    clf_l1.fit(X_train, y_train)\n",
    "    y_pred_tr = clf_l1.best_estimator_.predict(X_train)\n",
    "    y_pred_te = clf_l1.best_estimator_.predict(X_test)\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train, y_pred_tr))\n",
    "    train_precision.append(precision_score(y_train, y_pred_tr))\n",
    "    train_recall.append(recall_score(y_train, y_pred_tr))\n",
    "    train_f1.append(f1_score(y_train, y_pred_tr))\n",
    "    train_auc.append(roc_auc_score(y_train, y_pred_tr))\n",
    "    \n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred_te))\n",
    "    test_precision.append(precision_score(y_test, y_pred_te))\n",
    "    test_recall.append(recall_score(y_test, y_pred_te))\n",
    "    test_f1.append(f1_score(y_test, y_pred_te))\n",
    "    test_auc.append(roc_auc_score(y_test, y_pred_te))\n",
    "    \n",
    "avg_acc_tr = np.mean(train_accuracy)\n",
    "avg_pre_tr = np.mean(train_precision)\n",
    "avg_rec_tr = np.mean(train_recall)\n",
    "avg_f1_tr = np.mean(train_f1)\n",
    "avg_auc_tr = np.mean(train_auc)\n",
    "    \n",
    "avg_acc_te = np.mean(test_accuracy)\n",
    "avg_pre_te = np.mean(test_precision)\n",
    "avg_rec_te = np.mean(test_recall)\n",
    "avg_f1_te = np.mean(test_f1)\n",
    "avg_auc_te = np.mean(test_auc)\n",
    "    \n",
    "report_train_SL = [avg_acc_tr, avg_pre_tr, avg_rec_tr, avg_f1_tr, avg_auc_tr]\n",
    "report_test_SL = [avg_acc_te, avg_pre_te, avg_rec_te, avg_f1_te, avg_auc_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.982895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.986236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.967647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.976838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.979803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Supervised\n",
       "train_accuracy     0.982895\n",
       "train_precision    0.986236\n",
       "train_recall       0.967647\n",
       "train_f1           0.976838\n",
       "train_auc          0.979803"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_SL = pd.DataFrame(report_train_SL, index = train_rep, columns = ['Supervised'])\n",
    "train_SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.971681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.968019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.956349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.961551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_auc</th>\n",
       "      <td>0.968550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Supervised\n",
       "test_accuracy     0.971681\n",
       "test_precision    0.968019\n",
       "test_recall       0.956349\n",
       "test_f1           0.961551\n",
       "test_auc          0.968550"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_SL = pd.DataFrame(report_test_SL, index = test_rep, columns = ['Supervised'])\n",
    "test_SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO             284               2\n",
      "Actual: YES              5             165 \n",
      "\n",
      "Test Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO              68               3\n",
      "Actual: YES              1              41\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "svc_lin = clf_l1.best_estimator_\n",
    "\n",
    "train_proba = svc_lin.decision_function(X_train)\n",
    "test_proba = svc_lin.decision_function(X_test)\n",
    "train_fpr, train_tpr, train_threshold = roc_curve(y_train, train_proba)\n",
    "test_fpr, test_tpr, test_threshold = roc_curve(y_test, test_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_fpr, train_tpr, label = 'Train ROC')\n",
    "plt.plot(test_fpr, test_tpr, label = 'Test ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_train, svc_lin.predict(X_train))\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Training Confusion Matrix:')\n",
    "print(cm,'\\n')\n",
    "mat = confusion_matrix(y_test, svc_lin.predict(X_test))\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Test Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Semi-Supervised Learning / Self-training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_random_split_SSL(i):\n",
    "    b_df = df.loc[df['Class'] == 'B'].sample(frac = 0.2, random_state = i)\n",
    "    m_df = df.loc[df['Class'] == 'M'].sample(frac = 0.2, random_state = i)\n",
    "    df_test = b_df.append(m_df)\n",
    "    df_train = df.drop(df_test.index).reset_index(drop = True)\n",
    "    df_test = df_test.reset_index(drop = True)\n",
    "\n",
    "    b_df_train = df_train.loc[df_train['Class'] == 'B'].sample(frac = 0.5, random_state = i)\n",
    "    m_df_train = df_train.loc[df_train['Class'] == 'M'].sample(frac = 0.5, random_state = i)\n",
    "    df_train_labeled = b_df_train.append(m_df_train)\n",
    "    df_train_unlabeled = df_train.drop(df_train_labeled.index).reset_index(drop = True)\n",
    "    df_train_labeled = df_train_labeled.reset_index(drop = True)\n",
    "    \n",
    "    X_train_lbd = Normalizer().fit_transform(df_train_labeled.drop(['ID', 'Class'], axis = 1))\n",
    "    y_train_lbd = LabelBinarizer().fit_transform(df_train_labeled['Class'])\n",
    "    X_train_unlbd = Normalizer().fit_transform(df_train_unlabeled.drop(['ID', 'Class'], axis = 1))\n",
    "    y_train_unlbd = LabelBinarizer().fit_transform(df_train_unlabeled['Class'])\n",
    "    X_test = Normalizer().fit_transform(df_test.drop(['ID', 'Class'], axis = 1))\n",
    "    y_test = LabelBinarizer().fit_transform(df_test['Class'])\n",
    "    \n",
    "    return X_train_lbd, y_train_lbd, X_train_unlbd, y_train_unlbd, X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10000.0, dual=False, penalty='l1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split and normalized data\n",
    "X_train_lbd, y_train_lbd, X_train_unlbd, y_train_unlbd, X_test, y_test  = normalized_random_split_SSL(0)\n",
    "\n",
    "# train model with labeled data\n",
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "clf_l1.fit(X_train_lbd, y_train_lbd)\n",
    "\n",
    "# get result with the best C from grid search\n",
    "svc_lin = LinearSVC(C = clf_l1.best_params_['C'], penalty = 'l1', dual = False)\n",
    "svc_lin.fit(X_train_lbd, y_train_lbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and normalized data\n",
    "X_train_lbd, y_train_lbd, X_train_unlbd, y_train_unlbd, X_test, y_test  = normalized_random_split_SSL(0)\n",
    "\n",
    "# train model with labeled data\n",
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "clf_l1.fit(X_train_lbd, y_train_lbd)\n",
    "\n",
    "# use model to predict labels for unlabeled data and complete training set\n",
    "svc_lin = LinearSVC(C = clf_l1.best_params_['C'], penalty = 'l1', dual = False)\n",
    "svc_lin.fit(X_train_lbd, y_train_lbd)\n",
    "\n",
    "while len(X_train_unlbd) > 0:\n",
    "    train_proba_unlbd = svc_lin.decision_function(X_train_unlbd)\n",
    "    proba_abs = np.abs(train_proba_unlbd)\n",
    "    index = proba_abs.argmax()\n",
    "    X_train = np.vstack([X_train_lbd, X_train_unlbd[index]])\n",
    "    y_train = np.append(y_train_lbd, int(train_proba_unlbd[index] > 0))\n",
    "    X_train_unlbd = np.delete(X_train_unlbd, index, axis = 0)\n",
    "    svc_lin.fit(X_train_lbd, y_train_lbd)\n",
    "\n",
    "# run SSL for M = 30 times\n",
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "\n",
    "train_accuracy = []\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "train_f1 = []\n",
    "train_auc = []\n",
    "    \n",
    "test_accuracy = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "test_auc = []\n",
    "\n",
    "for i in range(0,30):\n",
    "    clf_l1.fit(X_train, y_train)\n",
    "    y_pred_tr = clf_l1.best_estimator_.predict(X_train)\n",
    "    y_pred_te = clf_l1.best_estimator_.predict(X_test)\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train, y_pred_tr))\n",
    "    train_precision.append(precision_score(y_train, y_pred_tr))\n",
    "    train_recall.append(recall_score(y_train, y_pred_tr))\n",
    "    train_f1.append(f1_score(y_train, y_pred_tr))\n",
    "    train_auc.append(roc_auc_score(y_train, y_pred_tr))\n",
    "    \n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred_te))\n",
    "    test_precision.append(precision_score(y_test, y_pred_te))\n",
    "    test_recall.append(recall_score(y_test, y_pred_te))\n",
    "    test_f1.append(f1_score(y_test, y_pred_te))\n",
    "    test_auc.append(roc_auc_score(y_test, y_pred_te))\n",
    "    \n",
    "avg_acc_tr = np.mean(train_accuracy)\n",
    "avg_pre_tr = np.mean(train_precision)\n",
    "avg_rec_tr = np.mean(train_recall)\n",
    "avg_f1_tr = np.mean(train_f1)\n",
    "avg_auc_tr = np.mean(train_auc)\n",
    "    \n",
    "avg_acc_te = np.mean(test_accuracy)\n",
    "avg_pre_te = np.mean(test_precision)\n",
    "avg_rec_te = np.mean(test_recall)\n",
    "avg_f1_te = np.mean(test_f1)\n",
    "avg_auc_te = np.mean(test_auc)\n",
    "    \n",
    "report_train_SSL = [avg_acc_tr, avg_pre_tr, avg_rec_tr, avg_f1_tr, avg_auc_tr]\n",
    "report_test_SSL = [avg_acc_te, avg_pre_te, avg_rec_te, avg_f1_te, avg_auc_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SemiSupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.977729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.979481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.960853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.970063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.974366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SemiSupervised\n",
       "train_accuracy         0.977729\n",
       "train_precision        0.979481\n",
       "train_recall           0.960853\n",
       "train_f1               0.970063\n",
       "train_auc              0.974366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_SSL = pd.DataFrame(report_train_SSL, index = train_rep, columns = ['SemiSupervised'])\n",
    "train_SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SemiSupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.974041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.934747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.966266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_auc</th>\n",
       "      <td>0.979343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SemiSupervised\n",
       "test_accuracy         0.974041\n",
       "test_precision        0.934747\n",
       "test_recall           1.000000\n",
       "test_f1               0.966266\n",
       "test_auc              0.979343"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_SSL = pd.DataFrame(report_test_SSL, index = test_rep, columns = ['SemiSupervised'])\n",
    "test_SSL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO             141               2\n",
      "Actual: YES              4              82 \n",
      "\n",
      "Test Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO              68               3\n",
      "Actual: YES              0              42\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "svc_lin = clf_l1.best_estimator_\n",
    "\n",
    "train_proba = svc_lin.decision_function(X_train)\n",
    "test_proba = svc_lin.decision_function(X_test)\n",
    "train_fpr, train_tpr, train_threshold = roc_curve(y_train, train_proba)\n",
    "test_fpr, test_tpr, test_threshold = roc_curve(y_test, test_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_fpr, train_tpr, label = 'Train ROC')\n",
    "plt.plot(test_fpr, test_tpr, label = 'Test ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_train, svc_lin.predict(X_train))\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Training Confusion Matrix:')\n",
    "print(cm,'\\n')\n",
    "mat = confusion_matrix(y_test, svc_lin.predict(X_test))\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Test Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Unsupervised Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# run k-means for n_nit = 10 times with random initial centroids to avoid being trapped in a local min\n",
    "X_train, y_train, X_test, y_test = normalized_random_split(0)\n",
    "kmeans = KMeans(n_clusters = 2, init = 'random', n_init = 10).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run USL for M = 30 times\n",
    "kmeans = KMeans(n_clusters = 2, init = 'random', n_init = 10)\n",
    "\n",
    "train_accuracy = []\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "train_f1 = []\n",
    "train_auc = []\n",
    "\n",
    "for i in range(0,30):\n",
    "    X_train, y_train, X_test, y_test = normalized_random_split(i)\n",
    "    kmeans.fit(X_train)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    dist = distance.cdist(X_train, centers, 'euclidean')\n",
    "    \n",
    "    # majority polling\n",
    "    poll_index_0 = np.argpartition(dist[:,0], 30)[:30]\n",
    "    poll_index_1 = np.argpartition(dist[:,1], 30)[:30]\n",
    "    cluster_0 = [i for i in range(len(kmeans.predict(X_train))) if kmeans.predict(X_train)[i] == 0]\n",
    "    cluster_1 = [i for i in range(len(kmeans.predict(X_train))) if kmeans.predict(X_train)[i] == 1]\n",
    "    y_pred_tr = y_train.copy()\n",
    "    y_pred_tr[poll_index_0] = np.argmax(np.bincount(y_train[poll_index_0].flatten()))\n",
    "    y_pred_tr[poll_index_1] = np.argmax(np.bincount(y_train[poll_index_1].flatten()))\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train, y_pred_tr))\n",
    "    train_precision.append(precision_score(y_train, y_pred_tr))\n",
    "    train_recall.append(recall_score(y_train, y_pred_tr))\n",
    "    train_f1.append(f1_score(y_train, y_pred_tr))\n",
    "    train_auc.append(roc_auc_score(y_train, y_pred_tr))\n",
    "    \n",
    "avg_acc_tr = np.mean(train_accuracy)\n",
    "avg_pre_tr = np.mean(train_precision)\n",
    "avg_rec_tr = np.mean(train_recall)\n",
    "avg_f1_tr = np.mean(train_f1)\n",
    "avg_auc_tr = np.mean(train_auc)\n",
    "    \n",
    "report_train_USL = [avg_acc_tr, avg_pre_tr, avg_rec_tr, avg_f1_tr, avg_auc_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unsupervised KMeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.996564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.995692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.995098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.995391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.996267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unsupervised KMeans\n",
       "train_accuracy              0.996564\n",
       "train_precision             0.995692\n",
       "train_recall                0.995098\n",
       "train_f1                    0.995391\n",
       "train_auc                   0.996267"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_USL = pd.DataFrame(report_train_USL, index = train_rep, columns = ['Unsupervised KMeans'])\n",
    "train_USL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAFlCAYAAAAOIeUsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcHUlEQVR4nO3dfYxd5X0n8O9vzYuTNG7KS6hr47WDHILZJm4zxSXtNmajbiFp67QKSqBqgDYidEMb9UUlbdWkVasqkbZNikJKUcQSlE2JukkKrWiijUqWSAkGI3lpMIVY0MAYSoypcFoUEZNn/5gZ7zDMy52ZO3Pv3PP5SBa+9xzf+7OPjL58ec5zqrUWAADoov8w6AEAAGBQhGEAADpLGAYAoLOEYQAAOksYBgCgs4RhAAA664RBffFpp53Wtm7dOqivBwCgI+69996nWmunz3ZsYGF469at2bdv36C+HgCAjqiqb8x1zDIJAAA6SxgGAKCzhGEAADprYGuGZ/Od73wn4+Pj+fa3vz3oUUbC+vXrs3nz5px44omDHgUAYCgNVRgeHx/Py1/+8mzdujVVNehx1rTWWo4cOZLx8fFs27Zt0OMAAAyloVom8e1vfzunnnqqINwHVZVTTz1Vyw4AMI+hCsNJBOE+8mcJADC/oQvDg3TkyJHs3LkzO3fuzPd///dn06ZNx18/99xz8/7affv25dd+7dcW9X1bt27ND/7gD+a1r31t3vjGN+Yb3/j/W+CNj49nz5492b59e84666y8973vfcEMd999d37iJ34iZ599dl7zmtfkXe96V5599tnF/YYBADpOGJ7m1FNPzf79+7N///5cddVV+fVf//Xjr0866aQcO3Zszl87NjaWa6+9dtHfeccdd+S+++7L7t2788d//MdJJtb7/vzP/3ze+ta35utf/3oeeuih/Nu//Vt+7/d+L0ny5JNP5uKLL86HPvShPPjgg3nggQdy4YUX5lvf+tbSfuMAAB21YBiuqhur6ptV9bU5jldVXVtVB6vqvqr64f6POTiXX355fuM3fiMXXHBBrrnmmtx99915wxvekB/6oR/KG97whjz44INJki996Uv56Z/+6STJH/zBH+SXfumXsnv37rzqVa/qKSSff/75OXToUJLkH/7hH7J+/fpcccUVSZJ169blwx/+cG688cY8++yzue6663LZZZfl/PPPTzKxHOJtb3tbzjjjjJX4IwAAGFm97CZxU5KPJrl5juMXJdk++WNXkr+Y/Oey/OHf3p8Djx9d7se8wI4f2JAP/My5i/51Dz30UL74xS9m3bp1OXr0aO68886ccMIJ+eIXv5jf/d3fzWc+85kX/Zp/+qd/yh133JFvfetbOfvss/Mrv/Ir825x9vnPfz5vfetbkyT3339/Xv/617/g+IYNG7Jly5YcPHgwX/va13LZZZct+vcBAMALLRiGW2t3VtXWeU7Zk+Tm1lpLcldVvaKqNrbWnujXkIN28cUXZ926dUmSZ555Jpdddlm+/vWvp6ryne98Z9Zf85a3vCUnn3xyTj755Lzyla/Mk08+mc2bN7/ovAsuuCBPPvlkXvnKV75gmcRsN7/N9T4AwDD7w7+9P0mWVEqutH7sM7wpyWPTXo9PvveiMFxVVya5Mkm2bNky74cO0x/Wy172suM///3f//1ccMEF+dznPpd//ud/zu7du2f9NSeffPLxn69bt27O9cZ33HFHXvayl+Xyyy/P+9///vzZn/1Zzj333Be1zUePHs1jjz2Ws846K+eee27uvffe7NmzZ/m/OQCAFdbv/9vfT/24gW62qrLNdmJr7YbW2lhrbez000/vw1evvmeeeSabNm1Kktx00019+cyXvOQl+chHPpKbb745Tz/9dN70pjfl2Wefzc03T6xMef755/Obv/mbufzyy/PSl740V199dT7xiU9k7969xz/jk5/8ZP7lX/6lL/MAAHRFP8LweJIzp73enOTxPnzuUPrt3/7t/M7v/E5+7Md+LM8//3zfPnfjxo255JJLct1116Wq8rnPfS5//dd/ne3bt+fVr3511q9fnz/5kz9Jkpxxxhm55ZZb8lu/9Vs5++yzc8455+TLX/5yNmzY0Ld5AAC6oCaW+i5w0sSa4b9rrf2nWY69JcnVSd6ciRvnrm2tnbfQZ46NjbV9+/a94L0HHngg55xzTm+T0xN/pgDAoL39L7+aJPn0u88fyPdX1b2ttbHZji24Zriq/irJ7iSnVdV4kg8kOTFJWmvXJ7k9E0H4YJJnk1zRn7EBAGBl9bKbxCULHG9J3tO3iQAAYJV4Ah0AAJ01dGG4lzXM9MafJQDA/IYqDK9fvz5HjhwR4vqgtZYjR45k/fr1gx4FAGBo9eOhG32zefPmjI+P5/Dhw4MeZSSsX79+1qfeAQAwYajC8Iknnpht27YNegwAADpiqJZJAADAahKGAQDoLGEYAIDOEoYBAOisobqBDgCAte9Tex/NrfsPHX994Imj2bFxwwAnmptmGACAvrp1/6EceOLo8dc7Nm7Inp2bBjjR3DTDAAD03Y6NG/Lpd58/6DEWpBkGAKCzNMMAACzJzLXBU4Z5jfBMmmEAAJZk5trgKcO8RngmzTAAAEu2VtYGz0UYBgBgUaaWR6yl5RBzsUwCAIBFmR6E18pyiLlohgEAWND0m+WmgvBaXh4xRTMMAMCCpt8sNwqN8BTNMABAh8y1HdpCRqkNnk4zDADQIXNth7aQUWqDp9MMAwB0zCg2vEslDAMAjLjZbn5jgmUSAAAjblRvfusHzTAAwBq30E1xo3rzWz9ohgEA1riFborTBs9NMwwAMAI0v0ujGQYAoLOEYQAAOssyCQCANWrqxjnbpS2dZhgAYI2aHoTdILc0mmEAgCG00HZpiS3T+kEzDAAwhBbaLi2xZVo/aIYBABapl9Z2ubS+q0MzDACwSL20tsul9V0dmmEAgCXQ2o4GYRgAYJrF3LjG2meZBADANG5c6xbNMAAwcpZzg5sb17pFMwwAjJzl3OCm9e0WzTAAsGbN1QBrd+mVZhgAWLPmaoC1u/RKMwwArGkaYJZDMwwAQGcJwwAAdJZlEgDAQCxn+7MpHn7BcmmGAYCBWM72Z1PcKMdyaYYBgFU11Qjb/oxhIAwDAKtiKgTvfeTpJMmubadodRm4nsJwVV2Y5M+TrEvy8dbaB2cc/94kn0yyZfIz/3tr7X/0eVYAYA2baoOnQvClu7YMeiRYOAxX1bok1yX5ySTjSe6pqttaawemnfaeJAdaaz9TVacnebCq/mdr7bkVmRoAGIjl3PRmWQTDqJcb6M5LcrC19vBkuL0lyZ4Z57QkL6+qSvI9SZ5OcqyvkwIAA7ecm97c7MYw6mWZxKYkj017PZ5k14xzPprktiSPJ3l5kre31r7blwkBgFU1X/ur3WXU9NIM1yzvtRmvfyrJ/iQ/kGRnko9W1Ys2/auqK6tqX1XtO3z48CJHBQBWw3ztr3aXUdNLMzye5MxprzdnogGe7ookH2yttSQHq+qRJK9Jcvf0k1prNyS5IUnGxsZmBmoAYAX1ut5X+0uX9BKG70myvaq2JTmU5B1JLp1xzqNJ3pTky1V1RpKzkzzcz0EBgBdbzA1t07c0m4/2ly5ZMAy31o5V1dVJvpCJrdVubK3dX1VXTR6/PskfJbmpqv4xE8sqrmmtPbWCcwMAyQseXrEQW5rBi/W0z3Br7fYkt8947/ppP388yX/t72gAsLDlbPU1CixpgOXp5QY6ABhay9nqaxRY0gDL43HMAKx5mlFgqYRhgBHSxSUDva6XBZiNZRIAI6SLSwYsEwCWQzMMsMZ4OhhA/2iGAdYYTwcD6B/NMMCQm9kEa38B+kczDDDkZjbB2l+A/tEMAwwB64ABBkMYBuijpW5ttveRp5NMPC53Jk0wwMoRhgH6aGpJw2L3vd217ZTs2bkpl+7askKTATAbYRgYKYN+6IQlDQBrixvogJEy6IdOWNIAsLZohoGhtZSWVzMLwGJohoGhtZSWVzMLwGJohoGBm6sB1vICsNKEYWAgpgfgubYV0/ICsNKEYWAgpm9BZlsxAAZFGAYGxhIIAAbNDXQAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBn2VoNWFVTD9uY2mMYAAZJMwysqulB2NPlABg0zTCwKmY2wh62AcAw0AwDq0IjDMAw0gwDfTXVAM+kEQZgGAnDwJLNFnz3PvJ0kmTXtlNe8L5GGIBhJAwDSzbbrhC7tp2SPTs35dJdWwY4GQD0RhgGlsXSBwDWMjfQAQDQWZphYNE8OAOAUaEZBhbNNmkAjArNMLCgmbtG2CYNgFEhDAMvMjP8ztwuTSMMwKgQhoEXmbke2HZpAIwqYRg6Zq4nxE1nGQQAXeEGOuiYqdZ3PpZBANAVmmHoiJnboWl9AUAzDJ1hOzQAeDHNMHSIRhgAXkgYhhHnaXEAMDfLJGDEWR4BAHPTDMOIcsMcACxMMwwjSiMMAAvTDMMaN9dDNDTCALAwzTCscXM9REMjDAAL0wzDCNAAA8DS9BSGq+rCJH+eZF2Sj7fWPjjLObuTfCTJiUmeaq29sW9TQkfMteRhPrZMA4ClW3CZRFWtS3JdkouS7EhySVXtmHHOK5J8LMnPttbOTXJx/0eF0TfXkof5WA4BAEvXSzN8XpKDrbWHk6SqbkmyJ8mBaedcmuSzrbVHk6S19s1+DwqjzDZoADAYvdxAtynJY9Nej0++N92rk3xfVX2pqu6tqnfO9kFVdWVV7auqfYcPH17axDCCbIMGAIPRSzNcs7zXZvmc1yd5U5KXJPlqVd3VWnvoBb+otRuS3JAkY2NjMz8DRtp864E1wgAwGL00w+NJzpz2enOSx2c55/OttX9vrT2V5M4kr+vPiDAa5lsPrBEGgMHopRm+J8n2qtqW5FCSd2RijfB0tyb5aFWdkOSkJLuSfLifg8JaZT0wAAyvBcNwa+1YVV2d5AuZ2Frtxtba/VV11eTx61trD1TV55Pcl+S7mdh+7WsrOTgMwlK2Ptv7yNNJkl3bTtH+AsCQqdYGs3R3bGys7du3byDfDUv19r/86pL29d2zc1Mu3bVlhaYCAOZTVfe21sZmO+YJdDCPmU2wpQ4AMFp6uYEOOmvmTW9udAOA0aIZhgVoggFgdGmGAQDoLGEYAIDOskyCzlnM9mhL2TkCAFg7NMN0znxPgpvJDXMAMNo0w4ysuRpg26MBAFM0w4ysuRpgbS8AMEUzzMiZaoQ1wADAQjTDjJzpQVgDDADMRzPMmrCUHSA0wgDAQjTDrAl2gAAAVoJmmDVD2wsA9JswzFCauSzCwy8AgJVgmQRDaeayCEsfAICVoBlmoDwYAwAYJGGYgZgKwXsfeTpJsmvbKS84rgkGAFaDMMxATC2D2LXtlOzZuSmX7toy6JEAgA4Shlk105dEWAYBAAwDN9CxaqbfFGcZBAAwDDTDzGsxT35biDYYABg2mmHmtZgnvy1EGwwADBvNMAvS5gIAo0ozDABAZwnDAAB0lmUSJFn4SXAAAKNIM0ySuW+Uc9MbADDKNMMc50Y5AKBrNMMAAHSWMAwAQGcJwwAAdJYwDABAZ7mBrqNmbqVmCzUAoIs0wx01cys1W6gBAF2kGR4hcz04YzZTTbCt1ACALtMMj5C5HpwxG00wAIBmeOhpewEAVo4wPGALhd29jzydJNm17ZQFP0vbCwCwOMLwgE0tbZhrJ4dd207Jnp2bcumuLas8GQDA6BOGh4ClDQAAg+EGOgAAOksYBgCgs4RhAAA6SxgGAKCzhGEAADpLGAYAoLNsrbbKZj5kY749hgEAWFma4VU29ZCNKZ4aBwAwOJrhVTC9DZ5qgj1kAwBg8Hpqhqvqwqp6sKoOVtX75jnvR6rq+ap6W/9GXPumt8GaYACA4bFgM1xV65Jcl+Qnk4wnuaeqbmutHZjlvA8l+cJKDLrWaYMBAIZPL83weUkOttYebq09l+SWJHtmOe9Xk3wmyTf7OB8AAKyYXsLwpiSPTXs9PvnecVW1KcnPJbl+vg+qqiural9V7Tt8+PBiZwUAgL7qJQzXLO+1Ga8/kuSa1trz831Qa+2G1tpYa23s9NNP73HEtetTex/N2//yqy/YPQIAgOHRy24S40nOnPZ6c5LHZ5wzluSWqkqS05K8uaqOtdb+ph9DrlVTN865aQ4AYDj1EobvSbK9qrYlOZTkHUkunX5Ca23b1M+r6qYkf9flIDy1lZpt1AAAhtuCYbi1dqyqrs7ELhHrktzYWru/qq6aPD7vOuEu0ggDAKwNPT10o7V2e5LbZ7w3awhurV2+/LHWJo0wAMDa4nHMfaQRBgBYWzyOuc80wgAAa4dmGACAzhKGAQDoLGEYAIDOEoYBAOgsYRgAgM6ym0QfzNxfGACAtUEz3Af2FwYAWJs0w31if2EAgLVHMwwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnnTDoAdaiT+19NLfuP3T89YEnjmbHxg0DnAgAgKXQDC/BrfsP5cATR4+/3rFxQ/bs3DTAiQAAWArN8BLt2Lghn373+YMeAwCAZdAMAwDQWcIwAACdJQwDANBZwjAAAJ0lDAMA0FnCMAAAnWVrtUWYetiGh2wAAIwGzfAiTA/CHrIBALD2aYYXycM2AABGhzC8gKmlEUksjwAAGDGWSSxgamlEEssjAABGjGa4B5ZGAACMJs0wAACdJQwDANBZwjAAAJ0lDAMA0FnCMAAAnSUMAwDQWcIwAACdJQwDANBZwjAAAJ0lDAMA0FnCMAAAnSUMAwDQWcIwAACddcKgBxhWn9r7aG7dfygHnjiaHRs3DHocAABWQE/NcFVdWFUPVtXBqnrfLMd/oarum/zxlap6Xf9HXV3Tg/CenZsGPQ4AACtgwWa4qtYluS7JTyYZT3JPVd3WWjsw7bRHkryxtfavVXVRkhuS7FqJgVfKVBM8ZSoIf/rd5w9wKgAAVlIvzfB5SQ621h5urT2X5JYke6af0Fr7SmvtXydf3pVkc3/HXHlTTfAUjTAAwOjrZc3wpiSPTXs9nvlb319O8vfLGWpQNMEAAN3SSxiuWd5rs55YdUEmwvCPz3H8yiRXJsmWLVt6HBEAAFZGL8skxpOcOe315iSPzzypql6b5ONJ9rTWjsz2Qa21G1prY621sdNPP30p8wIAQN/0EobvSbK9qrZV1UlJ3pHktuknVNWWJJ9N8outtYf6PyYAAPTfgsskWmvHqurqJF9Isi7Jja21+6vqqsnj1yd5f5JTk3ysqpLkWGttbOXGBgCA5evpoRuttduT3D7jveun/fxdSd7V39EAAGBleRwzAACdJQwDANBZPS2TGGVTT56beuIcAADd0flmeHoQ9sQ5AIBu6XwznHjyHABAV3W+GQYAoLuEYQAAOksYBgCgs4RhAAA6SxgGAKCzhGEAADpLGAYAoLOEYQAAOksYBgCgs4RhAAA6q3OPY/7U3kdz6/5Dx18feOJodmzcMMCJAAAYlM41w7fuP5QDTxw9/nrHxg3Zs3PTACcCAGBQOtcMJxMB+NPvPn/QYwAAMGCda4YBAGCKMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGcJwwAAdFZPYbiqLqyqB6vqYFW9b5bjVVXXTh6/r6p+uP+jAgBAfy0YhqtqXZLrklyUZEeSS6pqx4zTLkqyffLHlUn+os9zAgBA3/XSDJ+X5GBr7eHW2nNJbkmyZ8Y5e5Lc3CbcleQVVbWxz7MCAEBfndDDOZuSPDbt9XiSXT2csynJE9NPqqorM9EcZ8uWLYudtS92/MCGgXwvAADDp5cwXLO815ZwTlprNyS5IUnGxsZedHw1fOBnzh3E1wIAMIR6WSYxnuTMaa83J3l8CecAAMBQ6SUM35Nke1Vtq6qTkrwjyW0zzrktyTsnd5X40STPtNaemPlBAAAwTBZcJtFaO1ZVVyf5QpJ1SW5srd1fVVdNHr8+ye1J3pzkYJJnk1yxciMDAEB/9LJmOK212zMReKe/d/20n7ck7+nvaAAAsLI8gQ4AgM4ShgEA6CxhGACAzhKGAQDoLGEYAIDOEoYBAOgsYRgAgM4ShgEA6CxhGACAzqqJh8cN4IurDif5xkC+PDktyVMD+m5Wh2vcDa5zN7jO3eA6j75BXuP/2Fo7fbYDAwvDg1RV+1prY4Oeg5XjGneD69wNrnM3uM6jb1ivsWUSAAB0ljAMAEBndTUM3zDoAVhxrnE3uM7d4Dp3g+s8+obyGndyzTAAACTdbYYBAGB0w3BVXVhVD1bVwap63yzHq6qunTx+X1X98CDmZHl6uM6/MHl976uqr1TV6wYxJ8uz0HWedt6PVNXzVfW21ZyP5evlGlfV7qraX1X3V9X/We0ZWb4e/p39vVX1t1X1fyev8xWDmJOlq6obq+qbVfW1OY4PXf4ayTBcVeuSXJfkoiQ7klxSVTtmnHZRku2TP65M8herOiTL1uN1fiTJG1trr03yRxnS9UrMrcfrPHXeh5J8YXUnZLl6ucZV9YokH0vys621c5NcvNpzsjw9/l1+T5IDrbXXJdmd5E+r6qRVHZTluinJhfMcH7r8NZJhOMl5SQ621h5urT2X5JYke2acsyfJzW3CXUleUVUbV3tQlmXB69xa+0pr7V8nX96VZPMqz8jy9fL3OUl+NclnknxzNYejL3q5xpcm+Wxr7dEkaa25zmtPL9e5JXl5VVWS70nydJJjqzsmy9FauzMT120uQ5e/RjUMb0ry2LTX45PvLfYchttir+EvJ/n7FZ2IlbDgda6qTUl+Lsn1qzgX/dPL3+VXJ/m+qvpSVd1bVe9ctenol16u80eTnJPk8ST/mOS9rbXvrs54rJKhy18nDPLLV1DN8t7MbTN6OYfh1vM1rKoLMhGGf3xFJ2Il9HKdP5Lkmtba8xOFEmtML9f4hCSvT/KmJC9J8tWququ19tBKD0ff9HKdfyrJ/iT/JclZSf53VX25tXZ0hWdj9Qxd/hrVMDye5Mxprzdn4r8yF3sOw62na1hVr03y8SQXtdaOrNJs9E8v13ksyS2TQfi0JG+uqmOttb9ZlQlZrl7/nf1Ua+3fk/x7Vd2Z5HVJhOG1o5frfEWSD7aJfV8PVtUjSV6T5O7VGZFVMHT5a1SXSdyTZHtVbZtceP+OJLfNOOe2JO+cvKvxR5M801p7YrUHZVkWvM5VtSXJZ5P8ogZpzVrwOrfWtrXWtrbWtib5X0n+myC8pvTy7+xbk/znqjqhql6aZFeSB1Z5Tpanl+v8aCba/1TVGUnOTvLwqk7JShu6/DWSzXBr7VhVXZ2Ju8rXJbmxtXZ/VV01efz6JLcneXOSg0mezcR/jbKG9Hid35/k1CQfm2wNj7XWxgY1M4vX43VmDevlGrfWHqiqzye5L8l3k3y8tTbr1k0Mpx7/Lv9Rkpuq6h8z8b/Tr2mtPTWwoVm0qvqrTOwEclpVjSf5QJITk+HNX55ABwBAZ43qMgkAAFiQMAwAQGcJwwAAdJYwDABAZwnDAAB0ljAMAEBnCcMAAHSWMAwAQGf9P5Q3mzP6Db4RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO             285               1\n",
      "Actual: YES              1             169\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "import math\n",
    "\n",
    "train_proba = list()\n",
    "for i in range(len(y_pred_tr)):\n",
    "    train_proba.append(math.exp(dist[i,y_pred_tr[i]])/math.exp(sum(dist[i])))\n",
    "train_fpr, train_tpr, train_threshold = roc_curve(y_train, train_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_fpr, train_tpr, label = 'Train ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_train, y_pred_tr)\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Training Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run USL for M = 30 times\n",
    "kmeans = KMeans(n_clusters = 2, init = 'random', n_init = 10)\n",
    "test_accuracy = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "test_auc = []\n",
    "\n",
    "for i in range(0,30):\n",
    "    y_pred_te = list()\n",
    "    X_train, y_train, X_test, y_test = normalized_random_split(i)\n",
    "    kmeans.fit(X_test)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    dist = distance.cdist(X_test, centers, 'euclidean')\n",
    "    for y in range(len(X_test)):\n",
    "        if dist[i,0]<dist[i,1]:\n",
    "            y_pred_te.append(0)\n",
    "        else:\n",
    "            y_pred_te.append(1)\n",
    "\n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred_te))\n",
    "    test_precision.append(precision_score(y_test, y_pred_te))\n",
    "    test_recall.append(recall_score(y_test, y_pred_te))\n",
    "    test_f1.append(f1_score(y_test, y_pred_te))\n",
    "    test_auc.append(roc_auc_score(y_test, y_pred_te))\n",
    "    \n",
    "avg_acc_te = np.mean(test_accuracy)\n",
    "avg_pre_te = np.mean(test_precision)\n",
    "avg_rec_te = np.mean(test_recall)\n",
    "avg_f1_te = np.mean(test_f1)\n",
    "avg_auc_te = np.mean(test_auc)\n",
    "\n",
    "report_test_USL = [avg_acc_te, avg_pre_te, avg_rec_te, avg_f1_te, avg_auc_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unsupervised KMeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.185841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_auc</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unsupervised KMeans\n",
       "test_accuracy              0.500000\n",
       "test_precision             0.185841\n",
       "test_recall                0.500000\n",
       "test_f1                    0.270968\n",
       "test_auc                   0.500000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_USL = pd.DataFrame(report_test_USL, index = test_rep, columns = ['Unsupervised KMeans'])\n",
    "test_USL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO               0              71\n",
      "Actual: YES              0              42\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "test_proba = list()\n",
    "for i in range(len(y_pred_te)):\n",
    "    test_proba.append(math.exp(dist[i,y_pred_te[i]])/math.exp(sum(dist[i])))\n",
    "test_fpr, test_tpr, test_threshold = roc_curve(y_test, test_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(test_fpr, test_tpr, label = 'Test ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred_te)\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Test Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Spectral Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# run SC for M = 30 times\n",
    "spec = SpectralClustering(n_clusters = 2, gamma = 1)\n",
    "\n",
    "train_accuracy = []\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "train_f1 = []\n",
    "train_auc = []\n",
    "tr_pred_all = np.empty((0,456), int)\n",
    "\n",
    "for i in range(0,30):\n",
    "    X_train, y_train, X_test, y_test = normalized_random_split(i)\n",
    "    spec.fit(X_train)\n",
    "    labels = spec.labels_\n",
    "    \n",
    "    # majority polling\n",
    "    cluster_0 = [i for i in range(len(labels)) if labels[i] == 0]\n",
    "    cluster_1 = [i for i in range(len(labels)) if labels[i] == 1]\n",
    "    y_pred_tr = y_train.copy()\n",
    "    y_pred_tr[cluster_0] = np.argmax(np.bincount(y_train[cluster_0].flatten()))\n",
    "    y_pred_tr[cluster_1] = np.argmax(np.bincount(y_train[cluster_1].flatten()))\n",
    "    \n",
    "    train_accuracy.append(accuracy_score(y_train, y_pred_tr))\n",
    "    train_precision.append(precision_score(y_train, y_pred_tr))\n",
    "    train_recall.append(recall_score(y_train, y_pred_tr))\n",
    "    train_f1.append(f1_score(y_train, y_pred_tr))\n",
    "    train_auc.append(roc_auc_score(y_train, y_pred_tr))\n",
    "    \n",
    "    tr_pred_all = np.vstack((tr_pred_all, y_pred_tr.flatten()))\n",
    "    \n",
    "avg_acc_tr = np.mean(train_accuracy)\n",
    "avg_pre_tr = np.mean(train_precision)\n",
    "avg_rec_tr = np.mean(train_recall)\n",
    "avg_f1_tr = np.mean(train_f1)\n",
    "avg_auc_tr = np.mean(train_auc)\n",
    "    \n",
    "report_train_SC = [avg_acc_tr, avg_pre_tr, avg_rec_tr, avg_f1_tr, avg_auc_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unsupervised Spectral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.881287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.958751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.712353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.817215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.847027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unsupervised Spectral\n",
       "train_accuracy                0.881287\n",
       "train_precision               0.958751\n",
       "train_recall                  0.712353\n",
       "train_f1                      0.817215\n",
       "train_auc                     0.847027"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_SC = pd.DataFrame(report_train_SC, index = train_rep, columns = ['Unsupervised Spectral'])\n",
    "train_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO             282               4\n",
      "Actual: YES             51             119\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "proba_0 = [np.bincount(tr_pred_all[:,i])[0]/30 for i in range(456)]\n",
    "train_proba = list()\n",
    "for i in range(len(y_pred_tr)):\n",
    "    if y_pred_tr[i] == 0:\n",
    "        train_proba.append(proba_0[i])\n",
    "    else:\n",
    "        train_proba.append(1-proba_0[i])\n",
    "train_fpr, train_tpr, train_threshold = roc_curve(y_train, train_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_fpr, train_tpr, label = 'Train ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_train, y_pred_tr)\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Training Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SC for M = 30 times\n",
    "spec = SpectralClustering(n_clusters = 2, gamma = 1)\n",
    "\n",
    "test_accuracy = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "test_auc = []\n",
    "te_pred_all = np.empty((0,113), int)\n",
    "\n",
    "for i in range(0,30):\n",
    "    y_pred_te = list()\n",
    "    X_train, y_train, X_test, y_test = normalized_random_split(i)\n",
    "    y_pred_te = spec.fit_predict(X_test)\n",
    "\n",
    "    test_accuracy.append(accuracy_score(y_test, y_pred_te))\n",
    "    test_precision.append(precision_score(y_test, y_pred_te))\n",
    "    test_recall.append(recall_score(y_test, y_pred_te))\n",
    "    test_f1.append(f1_score(y_test, y_pred_te))\n",
    "    test_auc.append(roc_auc_score(y_test, y_pred_te))\n",
    "    \n",
    "    te_pred_all = np.vstack((te_pred_all, y_pred_te.flatten()))\n",
    "    \n",
    "avg_acc_te = np.mean(test_accuracy)\n",
    "avg_pre_te = np.mean(test_precision)\n",
    "avg_rec_te = np.mean(test_recall)\n",
    "avg_f1_te = np.mean(test_f1)\n",
    "avg_auc_te = np.mean(test_auc)\n",
    "\n",
    "report_test_SC = [avg_acc_te, avg_pre_te, avg_rec_te, avg_f1_te, avg_auc_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unsupervised Spectral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.608260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.657817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.599460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.601710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unsupervised Spectral\n",
       "train_accuracy                0.608260\n",
       "train_precision               0.657817\n",
       "train_recall                  0.576190\n",
       "train_f1                      0.599460\n",
       "train_auc                     0.601710"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_SC = pd.DataFrame(report_test_SC, index = train_rep, columns = ['Unsupervised Spectral'])\n",
    "test_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix:\n",
      "             Predicted: NO  Predicted: YES\n",
      "Actual: NO              69               2\n",
      "Actual: YES             10              32\n"
     ]
    }
   ],
   "source": [
    "# plot ROC and report confusion matrix for the last iteration\n",
    "proba_0 = [np.bincount(te_pred_all[:,i])[0]/30 for i in range(113)]\n",
    "test_proba = list()\n",
    "for i in range(len(y_pred_te)):\n",
    "    if y_pred_te[i] == 0:\n",
    "        test_proba.append(proba_0[i])\n",
    "    else:\n",
    "        test_proba.append(1-proba_0[i])\n",
    "test_fpr, test_tpr, test_threshold = roc_curve(y_test, test_proba)\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(test_fpr, test_tpr, label = 'Test ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred_te)\n",
    "cm = pd.DataFrame(mat, index = ['Actual: NO', 'Actual: YES'], columns = ['Predicted: NO', 'Predicted: YES'])\n",
    "print('Test Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supervised</th>\n",
       "      <th>Semi Supervised</th>\n",
       "      <th>Unsupervised KMeans</th>\n",
       "      <th>Unsupervised Spectral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.982895</td>\n",
       "      <td>0.977729</td>\n",
       "      <td>0.996564</td>\n",
       "      <td>0.881287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>0.986236</td>\n",
       "      <td>0.979481</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.958751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.967647</td>\n",
       "      <td>0.960853</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.712353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.976838</td>\n",
       "      <td>0.970063</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.817215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_auc</th>\n",
       "      <td>0.979803</td>\n",
       "      <td>0.974366</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>0.847027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Supervised  Semi Supervised  Unsupervised KMeans  \\\n",
       "train_accuracy     0.982895         0.977729             0.996564   \n",
       "train_precision    0.986236         0.979481             0.995692   \n",
       "train_recall       0.967647         0.960853             0.995098   \n",
       "train_f1           0.976838         0.970063             0.995391   \n",
       "train_auc          0.979803         0.974366             0.996267   \n",
       "\n",
       "                 Unsupervised Spectral  \n",
       "train_accuracy                0.881287  \n",
       "train_precision               0.958751  \n",
       "train_recall                  0.712353  \n",
       "train_f1                      0.817215  \n",
       "train_auc                     0.847027  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_tr = pd.DataFrame({'Supervised': report_train_SL, 'Semi Supervised': report_train_SSL,\n",
    "                         'Unsupervised KMeans': report_train_USL, 'Unsupervised Spectral': report_train_SC},\n",
    "                        index = train_rep)\n",
    "table_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supervised</th>\n",
       "      <th>Semi Supervised</th>\n",
       "      <th>Unsupervised KMeans</th>\n",
       "      <th>Unsupervised Spectral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.971681</td>\n",
       "      <td>0.974041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.608260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.968019</td>\n",
       "      <td>0.934747</td>\n",
       "      <td>0.185841</td>\n",
       "      <td>0.657817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.956349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.961551</td>\n",
       "      <td>0.966266</td>\n",
       "      <td>0.270968</td>\n",
       "      <td>0.599460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_auc</th>\n",
       "      <td>0.968550</td>\n",
       "      <td>0.979343</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.601710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Supervised  Semi Supervised  Unsupervised KMeans  \\\n",
       "test_accuracy     0.971681         0.974041             0.500000   \n",
       "test_precision    0.968019         0.934747             0.185841   \n",
       "test_recall       0.956349         1.000000             0.500000   \n",
       "test_f1           0.961551         0.966266             0.270968   \n",
       "test_auc          0.968550         0.979343             0.500000   \n",
       "\n",
       "                Unsupervised Spectral  \n",
       "test_accuracy                0.608260  \n",
       "test_precision               0.657817  \n",
       "test_recall                  0.576190  \n",
       "test_f1                      0.599460  \n",
       "test_auc                     0.601710  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_te = pd.DataFrame({'Supervised': report_test_SL, 'Semi Supervised': report_test_SSL,\n",
    "                         'Unsupervised KMeans': report_test_USL, 'Unsupervised Spectral': report_test_SC},\n",
    "                        index = test_rep)\n",
    "table_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Active Learning Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'../data/data_banknote_authentication.txt',\n",
    "                 names = ['variance','skewness','curtosis','entropy','class'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['class']\n",
    "X = df.drop(['class'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 472/1372, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Passive Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_10(X, y):\n",
    "    X.reset_index(drop = True)\n",
    "    y.reset_index(drop = True)\n",
    "    \n",
    "    idx = np.random.choice(len(X), 10, replace = False)\n",
    "    \n",
    "    X_pool = X.iloc[idx].reset_index(drop = True)\n",
    "    y_pool = y.iloc[idx].reset_index(drop = True)\n",
    "    \n",
    "    X_remain = X.drop(X.index[idx]).reset_index(drop = True)\n",
    "    y_remain = y.drop(y.index[idx]).reset_index(drop = True)\n",
    "        \n",
    "    return X_pool, y_pool, X_remain, y_remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initial pool of 10 random selected data points\n",
    "\n",
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "\n",
    "errors_pas = list()\n",
    "\n",
    "for k in range(50):    \n",
    "    error_list = list()\n",
    "    X_pool, y_pool, X_remain, y_remain = random_10(X_train, y_train)\n",
    "\n",
    "    for i in range(90):\n",
    "        clf_l1.fit(X_pool, y_pool)\n",
    "        best_C = clf_l1.best_params_['C']\n",
    "        \n",
    "        svc = LinearSVC(C = best_C, penalty = 'l1', dual = False)\n",
    "        svc.fit(X_pool, y_pool)\n",
    "        error_list.append(1 - svc.score(X_test, y_test))\n",
    "        \n",
    "        if i != 89:\n",
    "            X_pool_new, y_pool_new, X_remain_new, y_remain_new = random_10(X_remain, y_remain)\n",
    "            X_pool = X_pool.append(X_pool_new).reset_index(drop = True)\n",
    "            y_pool = y_pool.append(y_pool_new).reset_index(drop = True)\n",
    "            X_remain = X_remain_new\n",
    "            y_remain = y_remain_new\n",
    "            \n",
    "    errors_pas.append(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.216102</td>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.334746</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.247881</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.216102</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.139831  0.108051  0.101695  0.029661  0.031780  0.042373  0.040254   \n",
       "1   0.131356  0.036017  0.055085  0.046610  0.046610  0.023305  0.046610   \n",
       "2   0.131356  0.033898  0.042373  0.042373  0.038136  0.023305  0.014831   \n",
       "3   0.040254  0.031780  0.033898  0.016949  0.016949  0.016949  0.016949   \n",
       "4   0.125000  0.031780  0.038136  0.033898  0.027542  0.027542  0.027542   \n",
       "5   0.069915  0.065678  0.063559  0.055085  0.059322  0.036017  0.036017   \n",
       "6   0.216102  0.184322  0.036017  0.048729  0.038136  0.016949  0.016949   \n",
       "7   0.127119  0.108051  0.036017  0.061441  0.042373  0.038136  0.036017   \n",
       "8   0.040254  0.021186  0.029661  0.023305  0.025424  0.016949  0.016949   \n",
       "9   0.031780  0.021186  0.031780  0.016949  0.016949  0.025424  0.012712   \n",
       "10  0.016949  0.029661  0.033898  0.025424  0.023305  0.021186  0.014831   \n",
       "11  0.161017  0.063559  0.036017  0.023305  0.025424  0.033898  0.021186   \n",
       "12  0.444915  0.063559  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "13  0.091102  0.067797  0.067797  0.057203  0.031780  0.048729  0.023305   \n",
       "14  0.129237  0.023305  0.019068  0.036017  0.031780  0.019068  0.014831   \n",
       "15  0.091102  0.067797  0.063559  0.023305  0.036017  0.040254  0.012712   \n",
       "16  0.131356  0.025424  0.025424  0.069915  0.050847  0.052966  0.004237   \n",
       "17  0.137712  0.016949  0.025424  0.031780  0.027542  0.029661  0.036017   \n",
       "18  0.046610  0.019068  0.036017  0.040254  0.042373  0.023305  0.021186   \n",
       "19  0.025424  0.112288  0.038136  0.025424  0.016949  0.029661  0.025424   \n",
       "20  0.120763  0.082627  0.057203  0.012712  0.012712  0.012712  0.012712   \n",
       "21  0.021186  0.116525  0.023305  0.023305  0.033898  0.033898  0.016949   \n",
       "22  0.161017  0.067797  0.067797  0.029661  0.031780  0.029661  0.012712   \n",
       "23  0.016949  0.014831  0.012712  0.012712  0.012712  0.014831  0.014831   \n",
       "24  0.019068  0.023305  0.029661  0.016949  0.014831  0.016949  0.016949   \n",
       "25  0.065678  0.019068  0.016949  0.016949  0.014831  0.016949  0.025424   \n",
       "26  0.161017  0.188559  0.016949  0.016949  0.016949  0.016949  0.016949   \n",
       "27  0.137712  0.025424  0.029661  0.027542  0.061441  0.023305  0.023305   \n",
       "28  0.095339  0.040254  0.038136  0.033898  0.033898  0.033898  0.040254   \n",
       "29  0.334746  0.067797  0.061441  0.059322  0.057203  0.025424  0.023305   \n",
       "30  0.161017  0.063559  0.050847  0.048729  0.021186  0.016949  0.021186   \n",
       "31  0.074153  0.029661  0.029661  0.025424  0.031780  0.072034  0.065678   \n",
       "32  0.247881  0.038136  0.036017  0.021186  0.014831  0.029661  0.029661   \n",
       "33  0.216102  0.091102  0.019068  0.008475  0.016949  0.016949  0.016949   \n",
       "34  0.444915  0.016949  0.012712  0.012712  0.012712  0.012712  0.012712   \n",
       "35  0.097458  0.006356  0.016949  0.016949  0.016949  0.012712  0.014831   \n",
       "36  0.084746  0.091102  0.042373  0.040254  0.042373  0.029661  0.036017   \n",
       "37  0.146186  0.057203  0.044492  0.067797  0.065678  0.016949  0.023305   \n",
       "38  0.112288  0.014831  0.019068  0.016949  0.019068  0.023305  0.021186   \n",
       "39  0.110169  0.016949  0.014831  0.023305  0.021186  0.019068  0.019068   \n",
       "40  0.154661  0.050847  0.040254  0.033898  0.025424  0.029661  0.029661   \n",
       "41  0.161017  0.029661  0.019068  0.019068  0.019068  0.019068  0.019068   \n",
       "42  0.152542  0.029661  0.088983  0.014831  0.014831  0.014831  0.012712   \n",
       "43  0.033898  0.040254  0.033898  0.042373  0.012712  0.031780  0.019068   \n",
       "44  0.144068  0.146186  0.067797  0.069915  0.010593  0.014831  0.016949   \n",
       "45  0.213983  0.080508  0.033898  0.033898  0.021186  0.021186  0.014831   \n",
       "46  0.139831  0.131356  0.019068  0.014831  0.008475  0.008475  0.004237   \n",
       "47  0.152542  0.082627  0.033898  0.014831  0.025424  0.019068  0.023305   \n",
       "48  0.144068  0.078390  0.061441  0.027542  0.069915  0.014831  0.023305   \n",
       "49  0.250000  0.031780  0.074153  0.031780  0.031780  0.029661  0.036017   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.012712  0.019068  0.021186  ...  0.019068  0.012712  0.012712  0.012712   \n",
       "1   0.016949  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "2   0.012712  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "3   0.016949  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "4   0.021186  0.025424  0.021186  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "5   0.036017  0.038136  0.027542  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "6   0.016949  0.012712  0.014831  ...  0.019068  0.019068  0.012712  0.012712   \n",
       "7   0.038136  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "8   0.014831  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "9   0.014831  0.012712  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "10  0.023305  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "11  0.021186  0.019068  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "12  0.014831  0.012712  0.014831  ...  0.012712  0.012712  0.019068  0.012712   \n",
       "13  0.021186  0.025424  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "14  0.014831  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "15  0.014831  0.014831  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "16  0.029661  0.029661  0.004237  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "17  0.036017  0.016949  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "18  0.027542  0.021186  0.027542  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "19  0.025424  0.014831  0.031780  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "20  0.019068  0.012712  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "21  0.016949  0.016949  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "22  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "23  0.014831  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "24  0.016949  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "25  0.019068  0.019068  0.021186  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "26  0.027542  0.014831  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "27  0.023305  0.019068  0.038136  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "28  0.021186  0.027542  0.029661  ...  0.019068  0.019068  0.012712  0.012712   \n",
       "29  0.040254  0.019068  0.016949  ...  0.012712  0.012712  0.019068  0.019068   \n",
       "30  0.021186  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "31  0.063559  0.055085  0.036017  ...  0.019068  0.019068  0.019068  0.019068   \n",
       "32  0.029661  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "33  0.016949  0.004237  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "34  0.012712  0.014831  0.014831  ...  0.019068  0.019068  0.012712  0.012712   \n",
       "35  0.016949  0.016949  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "36  0.069915  0.038136  0.038136  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "37  0.023305  0.016949  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "38  0.012712  0.016949  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "39  0.016949  0.019068  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "40  0.031780  0.031780  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "41  0.019068  0.019068  0.016949  ...  0.019068  0.012712  0.012712  0.019068   \n",
       "42  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "43  0.038136  0.029661  0.029661  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "44  0.010593  0.016949  0.021186  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "45  0.012712  0.019068  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "46  0.023305  0.004237  0.004237  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "47  0.021186  0.014831  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "48  0.023305  0.019068  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "49  0.033898  0.027542  0.021186  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "1   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "2   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "3   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "4   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "5   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "6   0.019068  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "7   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "8   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "9   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "10  0.012712  0.012712  0.012712  0.019068  0.019068  0.019068  \n",
       "11  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "12  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "13  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "14  0.019068  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "15  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "16  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "17  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "18  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "19  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "20  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "21  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "22  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "23  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "24  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "25  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "26  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "27  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "28  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "29  0.019068  0.019068  0.019068  0.019068  0.019068  0.019068  \n",
       "30  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "31  0.019068  0.012712  0.012712  0.012712  0.019068  0.012712  \n",
       "32  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "33  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "34  0.012712  0.012712  0.019068  0.019068  0.012712  0.012712  \n",
       "35  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "36  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "37  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "38  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "39  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "40  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "41  0.019068  0.019068  0.012712  0.012712  0.012712  0.012712  \n",
       "42  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "43  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "44  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "45  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "46  0.012712  0.012712  0.012712  0.019068  0.019068  0.019068  \n",
       "47  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "48  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "49  0.019068  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive = pd.DataFrame(errors_pas)\n",
    "passive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Active Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'C':np.logspace(-4,4,9)}\n",
    "svc_lin = LinearSVC(penalty = 'l1', dual = False)\n",
    "clf_l1 = GridSearchCV(svc_lin, param_grid, cv = 5, n_jobs = -1)\n",
    "\n",
    "errors_act = list()\n",
    "\n",
    "for k in range(50):    \n",
    "    error_list = list()\n",
    "    X_pool, y_pool, X_remain, y_remain = random_10(X_train, y_train)\n",
    "\n",
    "    for i in range(90):\n",
    "        clf_l1.fit(X_pool, y_pool)\n",
    "        best_C = clf_l1.best_params_['C']\n",
    "        \n",
    "        svc = LinearSVC(C = best_C, penalty = 'l1', dual = False)\n",
    "        svc.fit(X_pool, y_pool)\n",
    "        error_list.append(1 - svc.score(X_test, y_test))\n",
    "        \n",
    "        if i != 89:\n",
    "            dist = svc.decision_function(X_remain)\n",
    "            index = np.argsort(np.abs(dist))[:10]\n",
    "            X_pool_new = X_remain.iloc[index, :]\n",
    "            y_pool_new = y_remain.iloc[index]\n",
    "            X_remain_new = X_remain.drop(X_remain.index[index])\n",
    "            y_remain_new = y_remain.drop(y_remain.index[index])\n",
    "            \n",
    "            X_pool = X_pool.append(X_pool_new).reset_index(drop = True)\n",
    "            y_pool = y_pool.append(y_pool_new).reset_index(drop = True)\n",
    "            X_remain = X_remain_new.reset_index(drop = True)\n",
    "            y_remain = y_remain_new.reset_index(drop = True)\n",
    "\n",
    "    \n",
    "    errors_act.append(error_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.233051</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.201271</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.224576</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.444915  0.112288  0.029661  0.010593  0.006356  0.010593  0.012712   \n",
       "1   0.372881  0.105932  0.093220  0.016949  0.052966  0.031780  0.019068   \n",
       "2   0.177966  0.029661  0.012712  0.012712  0.012712  0.012712  0.027542   \n",
       "3   0.118644  0.021186  0.044492  0.031780  0.012712  0.019068  0.012712   \n",
       "4   0.141949  0.016949  0.137712  0.014831  0.012712  0.012712  0.012712   \n",
       "5   0.156780  0.019068  0.021186  0.019068  0.012712  0.012712  0.012712   \n",
       "6   0.044492  0.021186  0.019068  0.010593  0.010593  0.012712  0.012712   \n",
       "7   0.080508  0.025424  0.031780  0.023305  0.021186  0.012712  0.012712   \n",
       "8   0.065678  0.233051  0.055085  0.012712  0.012712  0.019068  0.019068   \n",
       "9   0.044492  0.023305  0.021186  0.012712  0.012712  0.019068  0.012712   \n",
       "10  0.330508  0.074153  0.074153  0.057203  0.012712  0.008475  0.014831   \n",
       "11  0.038136  0.144068  0.063559  0.027542  0.010593  0.010593  0.038136   \n",
       "12  0.169492  0.029661  0.012712  0.019068  0.019068  0.019068  0.019068   \n",
       "13  0.133475  0.057203  0.038136  0.012712  0.014831  0.019068  0.014831   \n",
       "14  0.065678  0.036017  0.016949  0.021186  0.014831  0.014831  0.012712   \n",
       "15  0.118644  0.080508  0.014831  0.006356  0.019068  0.012712  0.019068   \n",
       "16  0.173729  0.088983  0.029661  0.019068  0.021186  0.014831  0.012712   \n",
       "17  0.129237  0.038136  0.010593  0.031780  0.016949  0.012712  0.012712   \n",
       "18  0.067797  0.014831  0.067797  0.025424  0.016949  0.012712  0.012712   \n",
       "19  0.139831  0.148305  0.029661  0.019068  0.014831  0.012712  0.012712   \n",
       "20  0.046610  0.014831  0.031780  0.027542  0.012712  0.012712  0.027542   \n",
       "21  0.144068  0.114407  0.063559  0.021186  0.021186  0.019068  0.012712   \n",
       "22  0.148305  0.019068  0.010593  0.010593  0.010593  0.010593  0.012712   \n",
       "23  0.129237  0.038136  0.021186  0.008475  0.012712  0.012712  0.019068   \n",
       "24  0.131356  0.125000  0.014831  0.016949  0.012712  0.012712  0.027542   \n",
       "25  0.127119  0.038136  0.025424  0.019068  0.012712  0.012712  0.019068   \n",
       "26  0.211864  0.036017  0.016949  0.006356  0.008475  0.012712  0.012712   \n",
       "27  0.084746  0.016949  0.033898  0.021186  0.006356  0.021186  0.012712   \n",
       "28  0.023305  0.175847  0.029661  0.019068  0.014831  0.012712  0.012712   \n",
       "29  0.144068  0.027542  0.012712  0.067797  0.016949  0.012712  0.012712   \n",
       "30  0.133475  0.044492  0.038136  0.008475  0.019068  0.012712  0.012712   \n",
       "31  0.067797  0.027542  0.095339  0.016949  0.019068  0.014831  0.010593   \n",
       "32  0.201271  0.031780  0.055085  0.038136  0.023305  0.025424  0.016949   \n",
       "33  0.224576  0.019068  0.171610  0.044492  0.038136  0.055085  0.008475   \n",
       "34  0.021186  0.057203  0.046610  0.025424  0.016949  0.012712  0.044492   \n",
       "35  0.144068  0.048729  0.019068  0.027542  0.027542  0.019068  0.019068   \n",
       "36  0.112288  0.021186  0.023305  0.014831  0.021186  0.012712  0.012712   \n",
       "37  0.161017  0.044492  0.029661  0.019068  0.019068  0.014831  0.012712   \n",
       "38  0.114407  0.052966  0.014831  0.023305  0.021186  0.021186  0.019068   \n",
       "39  0.163136  0.059322  0.014831  0.019068  0.012712  0.012712  0.012712   \n",
       "40  0.139831  0.019068  0.019068  0.019068  0.012712  0.012712  0.014831   \n",
       "41  0.161017  0.025424  0.019068  0.021186  0.019068  0.012712  0.014831   \n",
       "42  0.131356  0.061441  0.040254  0.012712  0.012712  0.012712  0.012712   \n",
       "43  0.019068  0.199153  0.023305  0.021186  0.012712  0.012712  0.012712   \n",
       "44  0.141949  0.158898  0.033898  0.016949  0.012712  0.012712  0.012712   \n",
       "45  0.112288  0.072034  0.016949  0.004237  0.019068  0.012712  0.019068   \n",
       "46  0.156780  0.040254  0.059322  0.029661  0.014831  0.008475  0.019068   \n",
       "47  0.118644  0.036017  0.023305  0.004237  0.006356  0.012712  0.019068   \n",
       "48  0.029661  0.048729  0.021186  0.012712  0.012712  0.012712  0.019068   \n",
       "49  0.044492  0.078390  0.027542  0.012712  0.012712  0.012712  0.019068   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "1   0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "2   0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "3   0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "4   0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "5   0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "6   0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "7   0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "8   0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "9   0.012712  0.019068  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "10  0.012712  0.019068  0.019068  ...  0.016949  0.016949  0.016949  0.012712   \n",
       "11  0.012712  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "12  0.012712  0.012712  0.019068  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "13  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "14  0.019068  0.012712  0.019068  ...  0.016949  0.012712  0.012712  0.012712   \n",
       "15  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.019068   \n",
       "16  0.019068  0.019068  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "17  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.012712   \n",
       "18  0.019068  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "19  0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "20  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.016949   \n",
       "21  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.019068  0.012712   \n",
       "22  0.012712  0.019068  0.012712  ...  0.016949  0.012712  0.012712  0.012712   \n",
       "23  0.012712  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.019068   \n",
       "24  0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "25  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "26  0.012712  0.019068  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "27  0.012712  0.019068  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "28  0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "29  0.019068  0.019068  0.012712  ...  0.016949  0.016949  0.012712  0.012712   \n",
       "30  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "31  0.012712  0.012712  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "32  0.019068  0.019068  0.019068  ...  0.016949  0.012712  0.016949  0.019068   \n",
       "33  0.012712  0.019068  0.019068  ...  0.019068  0.016949  0.016949  0.016949   \n",
       "34  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "35  0.019068  0.019068  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "36  0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "37  0.019068  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.012712   \n",
       "38  0.019068  0.019068  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "39  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "40  0.012712  0.019068  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "41  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "42  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "43  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "44  0.019068  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "45  0.012712  0.012712  0.012712  ...  0.016949  0.012712  0.019068  0.012712   \n",
       "46  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "47  0.012712  0.019068  0.012712  ...  0.012712  0.012712  0.016949  0.019068   \n",
       "48  0.019068  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "49  0.012712  0.012712  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.012712  0.012712  0.012712  0.012712  0.019068  0.012712  \n",
       "1   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "2   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "3   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "4   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "5   0.012712  0.012712  0.012712  0.019068  0.012712  0.012712  \n",
       "6   0.016949  0.016949  0.016949  0.012712  0.012712  0.012712  \n",
       "7   0.019068  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "8   0.012712  0.012712  0.012712  0.019068  0.019068  0.012712  \n",
       "9   0.016949  0.016949  0.016949  0.016949  0.012712  0.012712  \n",
       "10  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "11  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "12  0.019068  0.012712  0.012712  0.019068  0.012712  0.012712  \n",
       "13  0.012712  0.012712  0.012712  0.016949  0.016949  0.016949  \n",
       "14  0.012712  0.012712  0.019068  0.019068  0.012712  0.012712  \n",
       "15  0.019068  0.019068  0.019068  0.019068  0.019068  0.019068  \n",
       "16  0.012712  0.012712  0.012712  0.012712  0.019068  0.019068  \n",
       "17  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "18  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "19  0.012712  0.012712  0.012712  0.012712  0.016949  0.016949  \n",
       "20  0.016949  0.016949  0.016949  0.016949  0.012712  0.012712  \n",
       "21  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "22  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "23  0.019068  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "24  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "25  0.016949  0.016949  0.016949  0.016949  0.012712  0.016949  \n",
       "26  0.012712  0.012712  0.019068  0.019068  0.012712  0.012712  \n",
       "27  0.012712  0.012712  0.012712  0.019068  0.019068  0.012712  \n",
       "28  0.012712  0.012712  0.019068  0.019068  0.012712  0.019068  \n",
       "29  0.019068  0.019068  0.019068  0.012712  0.012712  0.012712  \n",
       "30  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "31  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "32  0.016949  0.016949  0.012712  0.012712  0.016949  0.016949  \n",
       "33  0.016949  0.016949  0.016949  0.012712  0.012712  0.012712  \n",
       "34  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "35  0.016949  0.016949  0.016949  0.019068  0.012712  0.012712  \n",
       "36  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "37  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "38  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "39  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "40  0.012712  0.019068  0.016949  0.012712  0.012712  0.019068  \n",
       "41  0.016949  0.016949  0.016949  0.016949  0.012712  0.012712  \n",
       "42  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "43  0.012712  0.012712  0.019068  0.012712  0.012712  0.012712  \n",
       "44  0.016949  0.016949  0.016949  0.019068  0.012712  0.012712  \n",
       "45  0.012712  0.019068  0.012712  0.012712  0.012712  0.012712  \n",
       "46  0.012712  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "47  0.016949  0.016949  0.012712  0.012712  0.016949  0.016949  \n",
       "48  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "49  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active = pd.DataFrame(errors_act)\n",
    "active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25ab9690eb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(np.arange(0,90)*10, passive.mean(), label = 'Passive Learning')\n",
    "plt.plot(np.arange(0,90)*10, active.mean(), label = 'Active Learning')\n",
    "plt.xlabel('Number of Training Instance')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as is shown in the learning curve, active learning has advantage over passive learning when the training data size is relatively small. however, this advantage will deminish as the training set grows larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
